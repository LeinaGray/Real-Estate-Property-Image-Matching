{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lamudi Scraper: Scrape Property Links and Details (title, price, description, etc.) from Lamudi Website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os, csv, time\n",
    "from bs4 import BeautifulSoup\n",
    "from unicodedata import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Prepare Lamudi Website and Browser to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseurl = \"https://www.lamudi.com.ph/buy/\"\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. Scrape all property links  listed in the Lamudi Webpage in a CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_links = []\n",
    "num_pages = 35\n",
    "timeout_value = 10\n",
    "for x in range(1,num_pages):\n",
    "    r = requests.get(f'https://www.lamudi.com.ph/buy/?page={x}, timeout={timeout_value}')\n",
    "    soup = BeautifulSoup(r.content, 'lxml')\n",
    "    property_rows = soup.find_all('div', class_='ListingCell-row')\n",
    "    # Iterate over each property row and extract links\n",
    "    for property_row in property_rows:\n",
    "        links = property_row.find_all('a', class_='ListingCell-ListingLink')  # Find all anchor tags within the row\n",
    "        for link in links:\n",
    "            href = link.get('href')  # Get the 'href' attribute of the link\n",
    "            if href:  # Check if the link has an 'href' attribute\n",
    "                property_links.append(href)\n",
    "                print(href)\n",
    "    print(len(property_links))\n",
    "    # Create a folder to store the images (if it doesn't exist)\n",
    "    folder_path = r\"dataset\\fraud\\evaluation-dataset\\property_listings_csv\"\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    file_path = os.path.join(folder_path, \"Lamudi_links.csv\")\n",
    "    # Create a CSV file\n",
    "    with open(file_path, \"w\", newline=\"\") as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow([\"Property Links\"])  # Write the header row\n",
    "        # Write each property link as a separate row\n",
    "        for link in property_links:\n",
    "            csv_writer.writerow([link])\n",
    "\n",
    "    print(f\"30 Property Links of page {x} written to Lamudi_links.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4. Save all property links in a python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r\"dataset\\fraud\\evaluation-dataset\\property_listings_csv\"\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "file_path = os.path.join(folder_path, \"Lamudi_links.csv\")\n",
    "with open(file_path, 'r') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    header = next(reader)  # Skip the header row\n",
    "    property_links = [row[0] for row in reader]  # Extract the first element (link) from each row\n",
    "\n",
    "print(property_links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5. Open each property link and scrape the title, price, address, bedrooms, bathrooms, floor area, description, list of amenities, and list of property image urls. Then, download the Porperty Image URLs in a folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "start_index = 0  # Starting index\n",
    "end_index = 1000    # Ending index (exclusive)\n",
    "for index in range(start_index, end_index+1):\n",
    "    # testlink = \"https://www.lamudi.com.ph/projects/one-rockwell/1-bedroom-loft-condo-for-sale-in-one-rockwell-maka-17164415131/\"\n",
    "    # testlink = property_links[1]\n",
    "    property_link = property_links[index - 1] \n",
    "    r = requests.get(property_link, headers = headers, timeout = 10)\n",
    "    soup = BeautifulSoup(r.content, 'lxml')\n",
    "    ID = index\n",
    "\n",
    "    title = soup.find('h1', class_='Title-pdp-title')\n",
    "    if title:\n",
    "        title = title.text.strip()\n",
    "        print(title)\n",
    "    else:\n",
    "        title = \"NA\" \n",
    "\n",
    "    address = soup.find('h3', class_='Title-pdp-address')\n",
    "    if address:\n",
    "        address = address.text.strip()\n",
    "        print(address)\n",
    "    else:\n",
    "        address = \"NA\"\n",
    "\n",
    "    price = soup.find('span', class_='FirstPrice')\n",
    "    print(price)\n",
    "    if price:\n",
    "        price = price.text.strip()\n",
    "        price = int(price.replace(\"â‚±\", \"\").replace(\",\", \"\"))\n",
    "        print(price)\n",
    "    else:\n",
    "        price = \"NA\"\n",
    "\n",
    "    attributes = soup.find('div', class_='Title-pdp-attributes')\n",
    "    if attributes:\n",
    "        attributes = attributes.text.strip().split()\n",
    "        if attributes:\n",
    "            bedrooms = attributes[0] if len(attributes) >= 1 else \"NA\"\n",
    "            bathrooms = attributes[1] if len(attributes) >= 2 else \"NA\"\n",
    "            floor_area = attributes[2] if len(attributes) >= 3 else \"NA\"\n",
    "            print(attributes)\n",
    "        else:\n",
    "            bedrooms = bathrooms = floor_area = \"NA\"\n",
    "    else:\n",
    "        bedrooms = bathrooms = floor_area = \"NA\"\n",
    "\n",
    "    description = soup.find('div', class_='ViewMore-text-description')\n",
    "    if description:\n",
    "        description = description.text.strip()\n",
    "        print(description)\n",
    "    else:\n",
    "        description = \"NA\"\n",
    "\n",
    "\n",
    "    amenities = soup.find_all('span', class_='listing-amenities-name')\n",
    "    if amenities:\n",
    "        amenities_list = [amenity.text.strip() for amenity in amenities]\n",
    "        amenities_string = \", \".join(amenities_list)\n",
    "        print(amenities_string)\n",
    "    else:\n",
    "        amenities_string = \"NA\"\n",
    "    \n",
    "    image_urls = soup.find_all('img', class_='jsGalleryMainImage', src=True)\n",
    "\n",
    "    if image_urls:\n",
    "        image_urls_string = \", \".join([url['src'] for url in image_urls])\n",
    "        folder_path = r\"dataset\\fraud\\evaluation-dataset\\copyright\\exif\"\n",
    "        os.makedirs(folder_path, exist_ok=True)\n",
    "        for num_img, image_url in enumerate(image_urls, start=1):\n",
    "            image_url = image_url['src']\n",
    "            print(image_url)\n",
    "            filename = f'A{index}.{num_img}.jpg'\n",
    "            print(filename)\n",
    "            # Construct the full path for the downloaded image\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            # Download the image\n",
    "            response = requests.get(image_url, stream=True)\n",
    "            if response.status_code == 200:\n",
    "                with open(file_path, 'wb') as f:\n",
    "                    for chunk in response.iter_content(1024):\n",
    "                        f.write(chunk)\n",
    "\n",
    "                print(f\"Image downloaded and saved as {filename}\")\n",
    "            else:\n",
    "                print(f\"Failed to download image. Status code: {response.status_code}\")\n",
    "        \n",
    "    data.append([ID, title, price, address, bedrooms, bathrooms, floor_area, description, amenities_string, image_urls_string])\n",
    "    csv = r'dataset\\fraud\\evaluation-dataset\\property_listings_csv\\Lamudi_properties.csv'\n",
    "    with open(csv, 'a', encoding='utf-8', newline='') as csvfile:\n",
    "        csv_writer = csv.writer(csvfile)\n",
    "        csv_writer.writerow([ID, title, price, address, bedrooms, bathrooms, floor_area, description, amenities_string, image_urls_string])\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example of donwloading only the Property Images in the Property link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testlink = \"https://www.lamudi.com.ph/projects/one-rockwell/1-bedroom-loft-condo-for-sale-in-one-rockwell-maka-17164415131/\"\n",
    "r = requests.get(testlink, headers = headers)\n",
    "soup = BeautifulSoup(r.content, 'lxml')\n",
    "image_urls = soup.find_all('img', class_='jsGalleryMainImage', src=True)\n",
    "# Create a folder to store the images (if it doesn't exist)\n",
    "folder_path = \"dataset/test_images\"\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "for num_img, image_url in enumerate(image_urls, start=1):\n",
    "    image_url = image_url['src']\n",
    "    print(image_url)\n",
    "    filename = f'{num_img}.jpg'\n",
    "    print(filename)\n",
    "    # Construct the full path for the downloaded image\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    # Download the image\n",
    "    response = requests.get(image_url, stream=True)\n",
    "    if response.status_code == 200:\n",
    "        with open(file_path, 'wb') as f:\n",
    "            for chunk in response.iter_content(1024):\n",
    "                f.write(chunk)\n",
    "\n",
    "        print(f\"Image downloaded and saved as {filename}\")\n",
    "    else:\n",
    "        print(f\"Failed to download image. Status code: {response.status_code}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
